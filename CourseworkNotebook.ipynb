{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to AI - Coursework\n",
    "Joshua Luke Boddy - aczc760\n",
    "\n",
    "Saeed Almansoori - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To-Do\n",
    "- Functions to prepare each column of the dataset we need\n",
    "- Decide on learning model (Multivariate regression?)\n",
    "- Research integer value return (Possible with Regression)\n",
    "\n",
    "## Data Preparation\n",
    "\n",
    "Taking a CSV file as our data set, the data needs to be converted across from CSV to something program readable as well as formatting the data into a readable format for our model (undecided).\n",
    "\n",
    "Columns needing data conversion:\n",
    "- Manufacturer Name -> Categorical\n",
    "- Model Name -> Categorical\n",
    "- Transmission -> Categorical\n",
    "- Color -> Categorical\n",
    "- Engine Fuel -> Categorical\n",
    "- Engine Has Gas -> Boolean\n",
    "- Engine Type -> Categorical\n",
    "- Body Type -> Categorical\n",
    "- Has Warranty -> Boolean\n",
    "- State -> Categorical\n",
    "- Drivetrain -> Categorical\n",
    "\n",
    "Extra Useable Columns\n",
    "- Odometer Value -> Int\n",
    "- Year Produced -> Int\n",
    "- Engine Capacity -> Float\n",
    "\n",
    "Data Labels\n",
    "- Price USD -> Float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Required Libraries\n",
    "\n",
    "The libraries below are required for the whole project and are imported at the start to maintain tidiness in the code, as well as make them usable throughout the notebook in later cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Data Set from CSV\n",
    "\n",
    "Reading the CSV file in to a data structure is simple using the pandas library as it has a built-in `read_csv` function that converts the data automatically (better than the alternative `with open(FILENAME, 'r') as file` option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manufacturer_name     object\n",
      "model_name            object\n",
      "transmission          object\n",
      "color                 object\n",
      "odometer_value         int64\n",
      "year_produced          int64\n",
      "engine_fuel           object\n",
      "engine_has_gas          bool\n",
      "engine_type           object\n",
      "engine_capacity      float64\n",
      "body_type             object\n",
      "has_warranty            bool\n",
      "state                 object\n",
      "drivetrain            object\n",
      "price_usd            float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "columnList = [\n",
    "    'manufacturer_name',\n",
    "    'model_name',\n",
    "    'transmission',\n",
    "    'color',\n",
    "    'engine_fuel',\n",
    "    'engine_has_gas',\n",
    "    'engine_type',\n",
    "    'body_type',\n",
    "    'has_warranty',\n",
    "    'state',\n",
    "    'drivetrain',\n",
    "    'odometer_value',\n",
    "    'year_produced',\n",
    "    'engine_capacity',\n",
    "    'price_usd'\n",
    "]\n",
    "\n",
    "data = pd.read_csv('./cars.csv', usecols=columnList)\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation Functions for Each Column\n",
    "\n",
    "Each column needs to be converted to a numerical data type so that it is readable by the neural network, this is done for each column below, however as opposed to writing functions for each column, we can determine what type the column is at the moment and write functions for each of those types respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert From Object\n",
    "def convertFromObject(column):\n",
    "    # This one liner takes a categorical column\n",
    "    # and assigns a value to each value in the column\n",
    "    # then returns the new column as a list of ints\n",
    "    return column.astype('category').cat.codes\n",
    "\n",
    "# Convert From Boolean\n",
    "def convertFromBoolean(column):\n",
    "    # Simply convert from boolean to integer\n",
    "    # where 0 is False and 1 is True\n",
    "    return column.astype(int)\n",
    "\n",
    "# Prepare Column function for each column\n",
    "# This contains the appropriate function for\n",
    "# each column in the dataset to run by\n",
    "# checking their data types\n",
    "def prepare_column(dtype, column):\n",
    "    if dtype == np.object:\n",
    "        return convertFromObject(column)\n",
    "    elif dtype == np.bool:\n",
    "        return convertFromBoolean(column)\n",
    "    elif dtype == np.int or dtype == np.float:\n",
    "        return column\n",
    "    else:\n",
    "        raise Exception(\"Data Type for Current Column %s not specified\" % (column.dtype))\n",
    "\n",
    "# The overall preparation function\n",
    "#Â This will prepare every column in the dataset\n",
    "# to be read by the neural network\n",
    "def prepare_dataset(dataset):\n",
    "    for column in dataset.columns:\n",
    "        dataset[column] = prepare_column(dataset[column].dtype, dataset[column])\n",
    "    labels = dataset.pop('price_usd')\n",
    "    labels /= 100000\n",
    "    print(labels)\n",
    "    print(labels.max())\n",
    "    return dataset.to_numpy(), labels.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Functions to Prepare the Data\n",
    "\n",
    "Below we are applying the functions defined in the previous section to our dataset to make it readable by the neural network. This is important as mathematical operations later won't be able to be performed on the data if it is not in readable format, and as such the network will not function at all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        0.109000\n",
      "1        0.050000\n",
      "2        0.028000\n",
      "3        0.099990\n",
      "4        0.021341\n",
      "           ...   \n",
      "38526    0.027500\n",
      "38527    0.048000\n",
      "38528    0.043000\n",
      "38529    0.040000\n",
      "38530    0.032000\n",
      "Name: price_usd, Length: 38531, dtype: float64\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "data, labels = prepare_dataset(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation\n",
    "TBD when we confirm whether we can use multivariate regression or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "        def __init__(self, layers=[14, 28, 28, 1], learningRate=0.001):\n",
    "            # Adjusted the code here so that the neural network parameters were customisable\n",
    "            # Same as before, hard coded values are stored in a dictionary and generated\n",
    "            # based on the sizes of the input, hidden and output layer sizes\n",
    "            self.layers = layers\n",
    "            self.learningRate = learningRate\n",
    "            self.params = {}\n",
    "            for i in range(1, len(self.layers)):\n",
    "                self.params['W' + str(i)] = np.random.uniform(-1, 1, size=(self.layers[i - 1], self.layers[i]))\n",
    "                self.params['B' + str(i)] = np.random.uniform(-1, 1, size=(self.layers[i]))\n",
    "        \n",
    "        def forwardPass(self, X):\n",
    "            params = self.params\n",
    "            params['A0'] = np.array(X)\n",
    "            for i in range(1, len(self.layers)):\n",
    "                params['Z' + str(i)] = np.dot(params['A' + str(i - 1)], params['W' + str(i)]) + params['B' + str(i)]\n",
    "                if(i == len(self.layers) - 1):\n",
    "                    params['A' + str(i)] = self.sigmoid(params['Z' + str(i)])\n",
    "                else:\n",
    "                    params['A' + str(i)] = self.sigmoid(params['Z' + str(i)])\n",
    "            return params[\"A\" + str(len(self.layers) - 1)]\n",
    "        \n",
    "        def backwardPass(self, Y, output):\n",
    "            params = self.params\n",
    "            changes = {}\n",
    "            placeholder = self.params\n",
    "            for i in reversed(range(1, len(self.layers))):\n",
    "                if (i == len(self.layers) - 1):\n",
    "                    error = 2 * (output - Y) / output.shape[0] * self.softmax(params['Z' + str(i)], derivative=True)\n",
    "                    changes['W' + str(i)] = np.outer(error, params['A' + str(i - 1)])\n",
    "                else:\n",
    "                    error = np.dot(error, params['W' + str(i + 1)].T) * self.sigmoid(params['Z' + str(i)], derivative=True)\n",
    "                    changes['W' + str(i)] = np.outer(error, params[\"A\" + str(i - 1)])\n",
    "            return changes\n",
    "        \n",
    "        def updateParams(self, changes):\n",
    "            for key, value in changes.items():\n",
    "                self.params[key] -= self.learningRate * np.transpose(value)\n",
    "        \n",
    "        def classify(self, X):\n",
    "            output = self.forwardPass(X)\n",
    "            return output * 100000\n",
    "            \n",
    "        @staticmethod   \n",
    "        def sigmoid(x, derivative = False):\n",
    "            if derivative:\n",
    "                return ((1/(1+np.exp(-x))) * (1 - 1/(1+np.exp(-x))))\n",
    "            return 1/(1+np.exp(-x))\n",
    "        \n",
    "        @staticmethod \n",
    "        def softmax(x, derivative = False):\n",
    "            exps = np.exp(x - x.max())\n",
    "            if derivative:\n",
    "                return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))\n",
    "            return exps / np.sum(exps, axis=0)\n",
    "        \n",
    "        \n",
    "        def train_function(X, Y, epochs = 5000 ):\n",
    "            \n",
    "            for i in range (0, epochs):\n",
    "                for j in range (0, len(data)):\n",
    "                    \n",
    "                    output = forwardPass (X)\n",
    "                    changes = backwardPass(output, Y)\n",
    "                    updateParams(changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "carsNN = NeuralNetwork()\n",
    "output = carsNN.forwardPass(data[0])\n",
    "changes = carsNN.backwardPass(labels[0], output)\n",
    "carsNN.updateParams(changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([97192.19599943])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carsNN.classify(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
